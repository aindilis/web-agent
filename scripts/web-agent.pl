#!/usr/bin/perl -w

# Takes a website, reads it's contents, tries to formalize it, etc.
# Make decisions using goals in our planning system.

# chooses what links to pursue


# when we have the webpage - obtain all the objects in it - like links
# and such


# try to extract metadata about the links, such as if there is a ul
# and it has a header, try to figure out what that header means and
# whether it applies to all the entries below it.


# web document layout understanding


# see about controlling/introspecting an actual browser, like selenium
# or something, also I think there was a Perl one

# %% importantKeywords(Keywords).


# use Prolog-Agent and NLU here





# combine natural language understanding with web crawling
